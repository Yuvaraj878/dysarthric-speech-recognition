{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4641357,"sourceType":"datasetVersion","datasetId":2698768}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch torchaudio transformers datasets jiwer --quiet","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:28:21.417407Z","iopub.execute_input":"2024-10-06T09:28:21.417888Z","iopub.status.idle":"2024-10-06T09:28:35.593531Z","shell.execute_reply.started":"2024-10-06T09:28:21.417840Z","shell.execute_reply":"2024-10-06T09:28:35.592179Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nimport torchaudio\nfrom datasets import Dataset, load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\nimport jiwer\n\n# Define paths to audio and transcription files\naudio_dir = \"/kaggle/input/dysarthria-and-nondysarthria-speech-dataset/Dysarthria and Non Dysarthria/Dataset/Female_Non_Dysarthria/FC01/Session1/Wav\"\ntranscription_dir = \"/kaggle/input/dysarthria-and-nondysarthria-speech-dataset/Dysarthria and Non Dysarthria/Dataset/Female_Non_Dysarthria/FC01/Session1/Txt\"\n\n# Function to load audio and transcription data\ndef load_data(audio_dir, transcription_dir):\n    audio_list = []\n    transcription_list = []\n    \n    for file in os.listdir(audio_dir):\n        if file.endswith(\".wav\"):\n            audio_path = os.path.join(audio_dir, file)\n            transcription_path = os.path.join(transcription_dir, file.replace(\".wav\", \".txt\"))\n\n            if os.path.exists(transcription_path):\n                with open(transcription_path, \"r\") as f:\n                    transcription = f.read().strip()\n\n                # Load audio file\n                waveform, sample_rate = torchaudio.load(audio_path)\n\n                # Append to lists\n                audio_list.append({\"array\": waveform.squeeze().numpy(), \"sampling_rate\": sample_rate})\n                transcription_list.append(transcription)\n\n    return {\"audio\": audio_list, \"transcription\": transcription_list}\n\n# Load dataset\ndata = load_data(audio_dir, transcription_dir)\n\n# Convert data to Hugging Face dataset format\ndataset = Dataset.from_dict(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:03:52.745532Z","iopub.execute_input":"2024-10-06T10:03:52.746665Z","iopub.status.idle":"2024-10-06T10:03:55.769888Z","shell.execute_reply.started":"2024-10-06T10:03:52.746614Z","shell.execute_reply":"2024-10-06T10:03:55.768899Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:32:20.332085Z","iopub.execute_input":"2024-10-06T09:32:20.332511Z","iopub.status.idle":"2024-10-06T09:32:20.342046Z","shell.execute_reply.started":"2024-10-06T09:32:20.332469Z","shell.execute_reply":"2024-10-06T09:32:20.340110Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['audio', 'transcription'],\n    num_rows: 164\n})"},"metadata":{}}]},{"cell_type":"code","source":"\n# Preprocess the dataset\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\ndef prepare_batch(batch):\n    # Extract audio and transcription\n    audio = batch[\"audio\"]\n    transcription = batch[\"transcription\"]\n    \n    # Preprocess audio\n    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\", padding=True)\n    \n    # Preprocess transcription\n    with processor.as_target_processor():\n        labels = processor(transcription, return_tensors=\"pt\").input_ids\n    \n    batch[\"input_values\"] = inputs.input_values[0]\n    batch[\"labels\"] = labels[0]\n    \n    return batch\n\n# Apply preprocessing to the dataset\ndataset = dataset.map(prepare_batch, remove_columns=[\"audio\", \"transcription\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:03:57.495674Z","iopub.execute_input":"2024-10-06T10:03:57.496219Z","iopub.status.idle":"2024-10-06T10:04:04.425640Z","shell.execute_reply.started":"2024-10-06T10:03:57.496162Z","shell.execute_reply":"2024-10-06T10:04:04.424611Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/164 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b3b09e78274bba894fe0706dcb84a9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate --quiet","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:34:26.938948Z","iopub.execute_input":"2024-10-06T09:34:26.939345Z","iopub.status.idle":"2024-10-06T09:34:39.156748Z","shell.execute_reply.started":"2024-10-06T09:34:26.939308Z","shell.execute_reply":"2024-10-06T09:34:39.155025Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Union\nimport numpy as np\nfrom transformers import Wav2Vec2Processor\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    processor: Wav2Vec2Processor\n    padding: bool = True\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Extract input values and labels\n        input_values = [feature[\"input_values\"] for feature in features]\n        labels = [feature[\"labels\"] for feature in features]\n\n        # Convert input_values to tensors if they're not already\n        input_values = [torch.tensor(x) if not isinstance(x, torch.Tensor) else x for x in input_values]\n\n        # Pad input values using the feature extractor\n        batch = self.processor.feature_extractor.pad(\n            {\"input_values\": input_values},\n            padding=self.padding,\n            return_tensors=\"pt\",\n        )\n\n        # Create attention mask if not provided\n        if \"attention_mask\" not in batch:\n            batch[\"attention_mask\"] = torch.ones_like(batch[\"input_values\"])\n\n        # Pad labels using the tokenizer\n        label_features = self.processor.tokenizer.pad(\n            {\"input_ids\": labels},\n            padding=self.padding,\n            return_tensors=\"pt\",\n        )\n\n        # Replace padding with -100 to ignore loss correctly\n        labels = label_features[\"input_ids\"].masked_fill(label_features.attention_mask.ne(1), -100)\n\n        batch[\"labels\"] = labels\n\n        return batch","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:07:50.012023Z","iopub.execute_input":"2024-10-06T10:07:50.012521Z","iopub.status.idle":"2024-10-06T10:07:50.027211Z","shell.execute_reply.started":"2024-10-06T10:07:50.012474Z","shell.execute_reply":"2024-10-06T10:07:50.025761Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n\n# Load your model and processor\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n\n# Create an instance of the data collator\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n\n# Set up the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2-torog\",\n    group_by_length=True,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy=\"steps\",\n    num_train_epochs=3,\n    fp16=True,\n    gradient_checkpointing=True,\n    save_steps=500,\n    eval_steps=500,\n    logging_steps=500,\n    learning_rate=1e-4,\n    weight_decay=0.005,\n    warmup_steps=1000,\n    save_total_limit=2,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    train_dataset=dataset,\n    eval_dataset=dataset,  # You might want to split your dataset into train and eval\n    tokenizer=processor.feature_extractor,\n)\n\n# Start training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:07:58.387546Z","iopub.execute_input":"2024-10-06T10:07:58.388065Z","iopub.status.idle":"2024-10-06T11:35:58.860127Z","shell.execute_reply.started":"2024-10-06T10:07:58.388019Z","shell.execute_reply":"2024-10-06T11:35:58.858440Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [33/33 1:20:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=33, training_loss=4602.848011363636, metrics={'train_runtime': 5273.1317, 'train_samples_per_second': 0.093, 'train_steps_per_second': 0.006, 'total_flos': 9.868901401169894e+16, 'train_loss': 4602.848011363636, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:37:30.691077Z","iopub.execute_input":"2024-10-06T11:37:30.692297Z","iopub.status.idle":"2024-10-06T11:37:30.707461Z","shell.execute_reply.started":"2024-10-06T11:37:30.692222Z","shell.execute_reply":"2024-10-06T11:37:30.706243Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2ForCTC(\n  (wav2vec2): Wav2Vec2Model(\n    (feature_extractor): Wav2Vec2FeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): Wav2Vec2GroupNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n          (activation): GELUActivation()\n          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n        )\n        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): Wav2Vec2FeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=1024, bias=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Wav2Vec2Encoder(\n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv): ParametrizedConv1d(\n          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n            (weight): ParametrizationList(\n              (0): _WeightNorm()\n            )\n          )\n        )\n        (padding): Wav2Vec2SamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0-23): 24 x Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2SdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model and processor\nmodel.save_pretrained(\"./wav2vec2-torog\")\nprocessor.save_pretrained(\"./wav2vec2-torog\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:39:01.888004Z","iopub.execute_input":"2024-10-06T11:39:01.888482Z","iopub.status.idle":"2024-10-06T11:39:04.628071Z","shell.execute_reply.started":"2024-10-06T11:39:01.888442Z","shell.execute_reply":"2024-10-06T11:39:04.626887Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport torchaudio\n\n# Load the model and processor\nmodel = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-torog\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"./wav2vec2-torog\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:39:20.772509Z","iopub.execute_input":"2024-10-06T11:39:20.772981Z","iopub.status.idle":"2024-10-06T11:39:21.034614Z","shell.execute_reply.started":"2024-10-06T11:39:20.772918Z","shell.execute_reply":"2024-10-06T11:39:21.033318Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"audio_file = \"/kaggle/input/dysarthria-and-nondysarthria-speech-dataset/Dysarthria and Non Dysarthria/Dataset/Female_Non_Dysarthria/FC01/Session1/Wav/0004.wav\"  # Replace with your audio file path\nwaveform, sample_rate = torchaudio.load(audio_file)\n\n# Preprocess the audio\ninputs = processor(waveform.squeeze().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:43:04.285881Z","iopub.execute_input":"2024-10-06T11:43:04.286612Z","iopub.status.idle":"2024-10-06T11:43:04.397132Z","shell.execute_reply.started":"2024-10-06T11:43:04.286551Z","shell.execute_reply":"2024-10-06T11:43:04.395660Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# Get the predicted logits\nwith torch.no_grad():\n    logits = model(inputs.input_values).logits\n\n# Get the predicted IDs\npredicted_ids = torch.argmax(logits, dim=-1)\n\n# Decode the predicted IDs to text\ntranscription = processor.batch_decode(predicted_ids)\nprint(\"Transcription:\", transcription)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:43:06.307094Z","iopub.execute_input":"2024-10-06T11:43:06.307534Z","iopub.status.idle":"2024-10-06T11:43:08.901822Z","shell.execute_reply.started":"2024-10-06T11:43:06.307495Z","shell.execute_reply":"2024-10-06T11:43:08.900511Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Transcription: ['EPAEPAEPA EPAEPA EPA EPA']\n","output_type":"stream"}]},{"cell_type":"code","source":"##############################","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:45:05.920371Z","iopub.execute_input":"2024-10-06T11:45:05.920946Z","iopub.status.idle":"2024-10-06T11:45:05.928061Z","shell.execute_reply.started":"2024-10-06T11:45:05.920897Z","shell.execute_reply":"2024-10-06T11:45:05.926992Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Zip the model directory\nshutil.make_archive('/kaggle/working/wav2vec2-torog', 'zip', '/kaggle/working/wav2vec2-torog')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:45:09.494887Z","iopub.execute_input":"2024-10-06T11:45:09.495338Z","iopub.status.idle":"2024-10-06T11:50:58.101134Z","shell.execute_reply.started":"2024-10-06T11:45:09.495292Z","shell.execute_reply":"2024-10-06T11:50:58.099781Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/wav2vec2-torog.zip'"},"metadata":{}}]},{"cell_type":"code","source":"import os\n\n# Check if the zip file exists\nzip_file_path = '/kaggle/working/wav2vec2-torog.zip'\nif os.path.exists(zip_file_path):\n    # Generate a download link\n    print(f'Download your model zip file here: {zip_file_path}')\nelse:\n    print(\"Zip file does not exist.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:53:05.784853Z","iopub.execute_input":"2024-10-06T11:53:05.786212Z","iopub.status.idle":"2024-10-06T11:53:05.794868Z","shell.execute_reply.started":"2024-10-06T11:53:05.786151Z","shell.execute_reply":"2024-10-06T11:53:05.793605Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Download your model zip file here: /kaggle/working/wav2vec2-torog.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}